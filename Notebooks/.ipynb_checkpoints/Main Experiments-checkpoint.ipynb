{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook runs simulations for our distributed algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Also, we only consider fixed-topology networks in the experiments below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "from matplotlib import style\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.dLdp import analytic_dLdp,analytic_dhdz,analytic_dhdq,analytic_FIM\n",
    "from utils.ConsensusEKF import ConsensusEKF\n",
    "from utils.CentralizedEKF import CentralizedEKF\n",
    "\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circulant(i,q,p,prev,post,undirected=False):\n",
    "    \"\"\"\n",
    "        Generate a circulant graph with len(p) nodes, node i connected with [i-prev:i+post],i-prev and i+post included but self-loop eliminated.\n",
    "    \"\"\"\n",
    "    n = len(p)\n",
    "    G = nx.DiGraph()\n",
    "    edges = [(j%n,i) for i in range(n) for j in range(i-prev,i+post+1)]\n",
    "    G.add_edges_from(edges)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    if undirected:\n",
    "        G = G.to_undirected()\n",
    "    return G\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_meas_func(C1,C0,k,b,dist):\n",
    "    return k*(dist-C1)**b+C0\n",
    "\n",
    "def joint_meas_func(C1s,C0s,ks,bs,x,ps):\n",
    "\n",
    "    # Casting for the compatibility of jax.numpy\n",
    "\n",
    "    C1=np.array(C1s)\n",
    "    C0=np.array(C0s)\n",
    "    k=np.array(ks)\n",
    "    b=np.array(bs)\n",
    "    p=np.array(ps)\n",
    "\n",
    "    # Keep in mind that x is a vector of [q,q'], thus only the first half of components are observable.    \n",
    "    dists=np.linalg.norm(x[:len(x)//2]-p,axis=1)\n",
    "\n",
    "    return single_meas_func(C1,C0,k,b,dists) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(q,N_trails,N_sen,N_iter,consensus_est=False,coordinate=False,version='v1',FIM_cons_iter=10):\n",
    "    '''Experiment Parameters'''\n",
    "     # Number of sensors.\n",
    "    comm_network_generator=lambda i,q,p:circulant(i,q,p,prev=1,post=0,undirected=True)\n",
    "    \n",
    "    C_gain=1/(2 * N_sen)\n",
    "\n",
    "    # Set up virtual sensors\n",
    "    C1=-0.3 # Setting C1 as a negative number mitigates the blowing-up effect when the sensors are close to the source.\n",
    "    C0=0\n",
    "    k=1\n",
    "    b=-2\n",
    "    noise_std = 0.01\n",
    "    minimum_sensing_reading=1e-5\n",
    "\n",
    "    # The communication network and consensus weight matrix.\n",
    "    p_0 = np.random.rand(N_sen,2)\n",
    "    G = comm_network_generator(0,q,p_0)\n",
    "    A = np.array(nx.adjacency_matrix(G).todense().astype(float))\n",
    "    A +=np.eye(len(A))\n",
    "\n",
    "    W = A/np.sum(A,axis=1) # The weight matrix required by parallel two-pass algorithm.\n",
    "\n",
    "    # The step size of each sensor\n",
    "    max_linear_speed=0.1\n",
    "\n",
    "    # Terminal condition\n",
    "    contact_radius = 0.1\n",
    "\n",
    "    def simulate_one_trial():\n",
    "        \n",
    "        '''Initialize Key Data Structures'''\n",
    "        p_0 = np.random.rand(N_sen,2)\n",
    "        qhat_0 = (np.random.rand(N_sen,2))*3\n",
    "\n",
    "\n",
    "        p = np.array(p_0) # Sensor Positins\n",
    "        qhat = np.array(qhat_0)\n",
    "\n",
    "        def F_single(dh,qhat,ps):\n",
    "            A = dh(qhat,ps)\n",
    "            return A.T.dot(A)\n",
    "\n",
    "        def joint_F_single(qhat,ps): # Verified to be correct.\n",
    "            # The vectorized version of F_single.\n",
    "            # The output shape is (N_sensor, q_dim, q_dim).\n",
    "            # Where output[i]=F_single(dh,qhat,ps[i])\n",
    "            A = analytic_dhdq(qhat,ps,C1s=C1,C0s=C0,ks=k,bs=b)\n",
    "            return A[:,np.newaxis,:]*A[:,:,np.newaxis]\n",
    "\n",
    "\n",
    "        # The list of single-term partial FIM's.\n",
    "        F_0 = joint_F_single(qhat,p)\n",
    "        F = np.array(F_0)\n",
    "\n",
    "        # The list local estimate of global FIM.\n",
    "        if version=='v1':\n",
    "            F_est = F+1e-8*np.eye(2) # Adding a small I to ensure invertibility\n",
    "        elif version=='v3':\n",
    "            local_FIM = np.zeros(F.shape)\n",
    "            for i in G.nodes():\n",
    "                N_i = [i]+list(G[i]) \n",
    "                local_FIM[i,:,:]=analytic_FIM(qhat[N_i,:],p[N_i],C1,C0,k,b)\n",
    "            F_est = local_FIM # v3: initialize F_est to be local FIMs.\n",
    "\n",
    "        # The Consensus EKFs\n",
    "        estimators = [ConsensusEKF(q_0,C_gain=C_gain) for q_0 in qhat_0]\n",
    "        \n",
    "\n",
    "        # The initialization of local measurement functions and the derivative functions. \n",
    "        # It is not very pretty. But is required by Consensus EKF.\n",
    "        hs = []\n",
    "        dhdzs = []\n",
    "        dhdqs = []\n",
    "        C1s=C1*np.ones(N_sen)\n",
    "        C0s = C0*np.ones(N_sen)\n",
    "        ks = k * np.ones(N_sen)\n",
    "        bs = b*np.ones(N_sen)\n",
    "\n",
    "        d = np.zeros(N_sen)\n",
    "        for i in G.nodes():  \n",
    "            N_i = [i]+list(G[i])     \n",
    "            C1s_i=C1s[N_i]\n",
    "            C0s_i = C0s[N_i]\n",
    "            ks_i = ks[N_i]\n",
    "            bs_i = bs[N_i]\n",
    "            hs.append(partial(joint_meas_func,C1s_i,C0s_i,ks_i,bs_i))# Freeze the coefficients, the signature becomes h(z,ps))\n",
    "            dhdzs.append(partial(analytic_dhdz,C1s=C1s_i,C0s=C0s_i,ks=ks_i,bs=bs_i))\n",
    "            dhdqs.append(partial(analytic_dhdq,C1s=C1s_i,C0s=C0s_i,ks=ks_i,bs=bs_i))\n",
    "            d[i]=len(N_i)\n",
    "\n",
    "        # Variables for parallel two-pass algorithm.\n",
    "        # Consensus on consensus weights.\n",
    "        inv_d = 1/d\n",
    "        w_F_est = F_est * inv_d[:,np.newaxis,np.newaxis]\n",
    "\n",
    "        '''Main Loop'''\n",
    "\n",
    "        p_history = []\n",
    "        qhat_history = []\n",
    "        q_history = []\n",
    "        for _ in range(N_iter):\n",
    "#             if np.min(np.linalg.norm(p-q,axis=1))>contact_radius:#Move and estimate only when not touching the source.\n",
    "            # Measure\n",
    "            r = np.linalg.norm(q-p,axis=1)\n",
    "            y = k* ((r-C1)**b)+C0 + np.random.randn(N_sen)*noise_std\n",
    "            y[y<=0]=minimum_sensing_reading # We don't want y to be zero or negative.\n",
    "\n",
    "\n",
    "            # Estimate\n",
    "            zhats = np.array([est.z for est in estimators])\n",
    "            new_qhat = np.zeros(qhat.shape)\n",
    "            local_FIM = np.zeros(F_est.shape)\n",
    "            for i in G.nodes():\n",
    "                N_i = [i]+list(G[i]) \n",
    "                # Estimate\n",
    "                if consensus_est:\n",
    "                    inv_d_neighbor=inv_d[N_i]\n",
    "                else:\n",
    "                    inv_d_neighbor=None\n",
    "                new_qhat[i,:]=estimators[i].update_and_estimate_loc(hs[i],dhdzs[i],y[N_i],p[N_i],zhats[N_i],consensus_weights=inv_d_neighbor)\n",
    "                if not coordinate:\n",
    "                    local_FIM[i,:,:]=analytic_FIM(qhat[N_i,:],p[N_i],C1,C0,k,b)\n",
    "\n",
    "            qhat=new_qhat\n",
    "\n",
    "            # Partial FIM Calculation and FIM consensus\n",
    "            for _ in range(FIM_cons_iter):\n",
    "                new_F = joint_F_single(qhat,p)\n",
    "                dF = new_F-F\n",
    "                F=new_F\n",
    "\n",
    "                # FIM Consensus using parallel two-pass algorithm\n",
    "                inv_d = W.dot(inv_d)\n",
    "                w_F_est = (w_F_est.T.dot(W)).T + dF*inv_d[:,np.newaxis,np.newaxis]\n",
    "                F_est = w_F_est/inv_d[:,np.newaxis,np.newaxis]   \n",
    "\n",
    "            # Gradient update\n",
    "            FIM_cand= F_est if coordinate else local_FIM\n",
    "            for i in range(N_sen):\n",
    "                dp=analytic_dLdp(qhat[i:i+1],p[i:i+1],C1,C0,k,b,FIM=FIM_cand[i])\n",
    "\n",
    "                p[i:i+1]-=max_linear_speed*dp/np.linalg.norm(dp)\n",
    "\n",
    "            # Record data\n",
    "            p_history.append(np.array(p))\n",
    "            qhat_history.append(np.array(qhat))\n",
    "            q_history.append(np.array(q))\n",
    "        \n",
    "        return {'p':p_history,'qhat':qhat_history,'q':q_history}\n",
    "        \n",
    "        \n",
    "    output = Parallel(n_jobs = 10)([delayed(simulate_one_trial)() for _ in range(N_trails)])\n",
    "\n",
    "    \n",
    "    data={'p':[],'qhat':[],'q':[]}\n",
    "    \n",
    "    for o in output:\n",
    "        for key, item in o.items():\n",
    "            data[key].append(item)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4v/bq7m33xj6b9fm2jfm0ngdg3c0000gn/T/ipykernel_3000/1927003437.py:19: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A = np.array(nx.adjacency_matrix(G).todense().astype(float))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 7.464532136917114\n",
      "Time 11.225635051727295\n",
      "Time 21.07914090156555\n",
      "Time 44.105435848236084\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel,delayed\n",
    "\n",
    "\n",
    "\n",
    "def get_args(N_sen):\n",
    "    return {'Coord.+Consensus Est.': (q,N_trails,N_sen,N_iter,True,True,'v1', 1),\n",
    "       'No Coord.+Local Est.':(q,N_trails,N_sen,N_iter,False,False,'v1'),\n",
    "       'No Coord.+Consensus Est.':(q,N_trails,N_sen,N_iter,True,False,'v1')}\n",
    "\n",
    "  \n",
    "SEED = 45\n",
    "np.random.seed(SEED)\n",
    "\n",
    "N_trails = 100\n",
    "N_iter = 150\n",
    "FIM_cons_iter = 1\n",
    "\n",
    "q = np.array([6,6])  \n",
    "\n",
    "sen_iter = [4,10,20,40]\n",
    "\n",
    "# sen_iter = [4,6,8]\n",
    "\n",
    "for N_sen in sen_iter:\n",
    "    t = time.time()\n",
    "   \n",
    "    args = {'Coord.+Consensus Est.': (q,N_trails,N_sen,N_iter,True,True,'v1', 1),\n",
    "           'No Coord.+Local Est.':(q,N_trails,N_sen,N_iter,False,False,'v1'),\n",
    "           'No Coord.+Consensus Est.':(q,N_trails,N_sen,N_iter,True,False,'v1')}\n",
    "    data = {}\n",
    "    for key,item in args.items():\n",
    "        data[key]=main(*item)\n",
    "\n",
    "\n",
    "    filepath = \"../Data/Distributed-FIM-{}Sensor.pkl\".format(N_sen)\n",
    "    with open(filepath,'wb') as file:\n",
    "        pkl.dump(data,file)\n",
    "\n",
    "    print('Time',time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralized Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralized_main(q,N_trails,N_sen,N_iter):\n",
    "    '''Experiment Parameters'''\n",
    "  \n",
    "    C_gain=0.1\n",
    "\n",
    "    # Set up virtual sensors\n",
    "    C1=-0.3 # Setting C1 as a negative number mitigates the blowing-up effect when the sensors are close to the source.\n",
    "    C0=0\n",
    "    k=1\n",
    "    b=-2\n",
    "    noise_std = 0.01\n",
    "    minimum_sensing_reading=1e-5\n",
    "\n",
    "    # The step size of each sensor\n",
    "    max_linear_speed=0.1\n",
    "\n",
    "    # Terminal condition\n",
    "    contact_radius = 0.1\n",
    "\n",
    "\n",
    "    t=time.time()\n",
    "    data={'p':[],'qhat':[],'q':[]}\n",
    "\n",
    "    for _ in range(N_trails):\n",
    "\n",
    "        '''Initialize Key Data Structures'''\n",
    "        p_0 = np.random.rand(N_sen,2)\n",
    "        qhat_0 = np.array([3,3])\n",
    "        # qhat_0 = np.random.rand(2)\n",
    "        \n",
    "        p = np.array(p_0) # Sensor Positins\n",
    "        qhat = np.array(qhat_0)\n",
    "\n",
    "        # The Centralized EKF\n",
    "        estimator = CentralizedEKF(qhat_0)\n",
    "\n",
    "        C1s=C1*np.ones(N_sen)\n",
    "        C0s = C0*np.ones(N_sen)\n",
    "        ks = k * np.ones(N_sen)\n",
    "        bs = b*np.ones(N_sen)\n",
    "\n",
    "        h=partial(joint_meas_func,C1s,C0s,ks,bs)# Freeze the coefficients, the signature becomes h(z,ps))\n",
    "        dhdz=partial(analytic_dhdz,C1s=C1s,C0s=C0s,ks=ks,bs=bs)\n",
    "        dhdqs=partial(analytic_dhdq,C1s=C1s,C0s=C0s,ks=ks,bs=bs)\n",
    "\n",
    "        '''Main Loop'''\n",
    "\n",
    "        p_history = []\n",
    "        qhat_history = []\n",
    "        q_history = []\n",
    "        for _ in range(N_iter):\n",
    "#             if np.min(np.linalg.norm(p-q,axis=1))>contact_radius:#Move and estimate only when not touching the source.\n",
    "            # Measure\n",
    "            r = np.linalg.norm(q-p,axis=1)\n",
    "            y = k* ((r-C1)**b)+C0 + np.random.randn(N_sen)*noise_std\n",
    "            y[y<=0]=minimum_sensing_reading # We don't want y to be zero or negative.\n",
    "\n",
    "\n",
    "            # Estimate\n",
    "\n",
    "            qhat=estimator.update_and_estimate_loc(h,dhdz,y,p)\n",
    "\n",
    "            # Gradient update\n",
    "\n",
    "            dp=analytic_dLdp(qhat,p,C1,C0,k,b)\n",
    "            p-=max_linear_speed*(dp.T/np.linalg.norm(dp,axis=1)).T\n",
    "\n",
    "            # Record data\n",
    "            p_history.append(np.array(p))\n",
    "            qhat_history.append(np.array(qhat))  \n",
    "            q_history.append(np.array(q))\n",
    "\n",
    "            # Check terminal condition\n",
    "            \n",
    "        data['q'].append(q_history)\n",
    "        data['p'].append(np.array(p_history))\n",
    "        data['qhat'].append(np.array(qhat_history))\n",
    "\n",
    "    print('Time:',time.time()-t)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.5410881042480469\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel,delayed\n",
    "\n",
    "def simulate(N_sen,N_trails,N_iter):\n",
    "    \n",
    "    SEED = 45\n",
    "    np.random.seed(SEED)\n",
    "    data={}\n",
    "    data['centralized']= centralized_main(q,N_trails,N_sen,N_iter)\n",
    "    filepath = \"../Data/CentralizedData-{}Sensor.pkl\".format(N_sen)\n",
    "    with open(filepath,'wb') as file:\n",
    "        pkl.dump(data,file)\n",
    "\n",
    "\n",
    "\n",
    "q = np.array([6,6])  \n",
    "N_trails = 100\n",
    "N_iter = 150\n",
    "\n",
    "\n",
    "# for N_sen in [4,6,8]:\n",
    "for N_sen in [4,10,20,40]:\n",
    "    simulate(N_sen,N_trails,N_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
