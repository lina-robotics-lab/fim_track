{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook runs simulations for our distributed algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Also, we only consider fixed-topology networks in the experiments below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     12\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdLdp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analytic_dLdp,analytic_dhdz,analytic_dhdq,analytic_FIM\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mConsensusEKF\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConsensusEKF\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCentralizedEKF\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CentralizedEKF\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "from matplotlib import style\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.dLdp import analytic_dLdp,analytic_dhdz,analytic_dhdq,analytic_FIM\n",
    "from utils.ConsensusEKF import ConsensusEKF\n",
    "from utils.CentralizedEKF import CentralizedEKF\n",
    "\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circulant(i,q,p,prev,post,undirected=False):\n",
    "    \"\"\"\n",
    "        Generate a circulant graph with len(p) nodes, node i connected with [i-prev:i+post],i-prev and i+post included but self-loop eliminated.\n",
    "    \"\"\"\n",
    "    n = len(p)\n",
    "    G = nx.DiGraph()\n",
    "    edges = [(j%n,i) for i in range(n) for j in range(i-prev,i+post+1)]\n",
    "    G.add_edges_from(edges)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    if undirected:\n",
    "        G = G.to_undirected()\n",
    "    return G\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_meas_func(C1,C0,k,b,dist):\n",
    "    return k*(dist-C1)**b+C0\n",
    "\n",
    "def joint_meas_func(C1s,C0s,ks,bs,x,ps):\n",
    "\n",
    "    # Casting for the compatibility of jax.numpy\n",
    "\n",
    "    C1=np.array(C1s)\n",
    "    C0=np.array(C0s)\n",
    "    k=np.array(ks)\n",
    "    b=np.array(bs)\n",
    "    p=np.array(ps)\n",
    "\n",
    "    # Keep in mind that x is a vector of [q,q'], thus only the first half of components are observable.    \n",
    "    dists=np.linalg.norm(x[:len(x)//2]-p,axis=1)\n",
    "\n",
    "    return single_meas_func(C1,C0,k,b,dists) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(q,N_trails,N_sen,N_iter,consensus_est=False,coordinate=False,version='v1',FIM_cons_iter=10):\n",
    "    '''Experiment Parameters'''\n",
    "     # Number of sensors.\n",
    "    comm_network_generator=lambda i,q,p:circulant(i,q,p,prev=1,post=0,undirected=True)\n",
    "    \n",
    "    C_gain=1/(2 * N_sen)\n",
    "\n",
    "    # Set up virtual sensors\n",
    "    C1=-0.3 # Setting C1 as a negative number mitigates the blowing-up effect when the sensors are close to the source.\n",
    "    C0=0\n",
    "    k=1\n",
    "    b=-2\n",
    "    noise_std = 0.01\n",
    "    minimum_sensing_reading=1e-5\n",
    "\n",
    "    # The communication network and consensus weight matrix.\n",
    "    p_0 = np.random.rand(N_sen,2)\n",
    "    G = comm_network_generator(0,q,p_0)\n",
    "    A = np.array(nx.adjacency_matrix(G).todense().astype(float))\n",
    "    A +=np.eye(len(A))\n",
    "\n",
    "    W = A/np.sum(A,axis=1) # The weight matrix required by parallel two-pass algorithm.\n",
    "\n",
    "    # The step size of each sensor\n",
    "    max_linear_speed=0.1\n",
    "\n",
    "    # Terminal condition\n",
    "    contact_radius = 0.1\n",
    "\n",
    "    def simulate_one_trial():\n",
    "        \n",
    "        '''Initialize Key Data Structures'''\n",
    "        p_0 = np.random.rand(N_sen,2)\n",
    "        qhat_0 = (np.random.rand(N_sen,2))*3\n",
    "\n",
    "\n",
    "        p = np.array(p_0) # Sensor Positins\n",
    "        qhat = np.array(qhat_0)\n",
    "\n",
    "        def F_single(dh,qhat,ps):\n",
    "            A = dh(qhat,ps)\n",
    "            return A.T.dot(A)\n",
    "\n",
    "        def joint_F_single(qhat,ps): # Verified to be correct.\n",
    "            # The vectorized version of F_single.\n",
    "            # The output shape is (N_sensor, q_dim, q_dim).\n",
    "            # Where output[i]=F_single(dh,qhat,ps[i])\n",
    "            A = analytic_dhdq(qhat,ps,C1s=C1,C0s=C0,ks=k,bs=b)\n",
    "            return A[:,np.newaxis,:]*A[:,:,np.newaxis]\n",
    "\n",
    "\n",
    "        # The list of single-term partial FIM's.\n",
    "        F_0 = joint_F_single(qhat,p)\n",
    "        F = np.array(F_0)\n",
    "\n",
    "        # The list local estimate of global FIM.\n",
    "        if version=='v1':\n",
    "            F_est = F+1e-8*np.eye(2) # Adding a small I to ensure invertibility\n",
    "        elif version=='v3':\n",
    "            local_FIM = np.zeros(F.shape)\n",
    "            for i in G.nodes():\n",
    "                N_i = [i]+list(G[i]) \n",
    "                local_FIM[i,:,:]=analytic_FIM(qhat[N_i,:],p[N_i],C1,C0,k,b)\n",
    "            F_est = local_FIM # v3: initialize F_est to be local FIMs.\n",
    "\n",
    "        # The Consensus EKFs\n",
    "        estimators = [ConsensusEKF(q_0,C_gain=C_gain) for q_0 in qhat_0]\n",
    "        \n",
    "\n",
    "        # The initialization of local measurement functions and the derivative functions. \n",
    "        # It is not very pretty. But is required by Consensus EKF.\n",
    "        hs = []\n",
    "        dhdzs = []\n",
    "        dhdqs = []\n",
    "        C1s=C1*np.ones(N_sen)\n",
    "        C0s = C0*np.ones(N_sen)\n",
    "        ks = k * np.ones(N_sen)\n",
    "        bs = b*np.ones(N_sen)\n",
    "\n",
    "        d = np.zeros(N_sen)\n",
    "        for i in G.nodes():  \n",
    "            N_i = [i]+list(G[i])     \n",
    "            C1s_i=C1s[N_i]\n",
    "            C0s_i = C0s[N_i]\n",
    "            ks_i = ks[N_i]\n",
    "            bs_i = bs[N_i]\n",
    "            hs.append(partial(joint_meas_func,C1s_i,C0s_i,ks_i,bs_i))# Freeze the coefficients, the signature becomes h(z,ps))\n",
    "            dhdzs.append(partial(analytic_dhdz,C1s=C1s_i,C0s=C0s_i,ks=ks_i,bs=bs_i))\n",
    "            dhdqs.append(partial(analytic_dhdq,C1s=C1s_i,C0s=C0s_i,ks=ks_i,bs=bs_i))\n",
    "            d[i]=len(N_i)\n",
    "\n",
    "        # Variables for parallel two-pass algorithm.\n",
    "        # Consensus on consensus weights.\n",
    "        inv_d = 1/d \n",
    "        w_F_est = F_est * inv_d[:,np.newaxis,np.newaxis]\n",
    "\n",
    "        '''Main Loop'''\n",
    "\n",
    "        p_history = []\n",
    "        qhat_history = []\n",
    "        q_history = []\n",
    "        for _ in range(N_iter):\n",
    "#             if np.min(np.linalg.norm(p-q,axis=1))>contact_radius:#Move and estimate only when not touching the source.\n",
    "            # Measure\n",
    "            r = np.linalg.norm(q-p,axis=1)\n",
    "            y = k* ((r-C1)**b)+C0 + np.random.randn(N_sen)*noise_std\n",
    "            y[y<=0]=minimum_sensing_reading # We don't want y to be zero or negative.\n",
    "\n",
    "\n",
    "            # Estimate\n",
    "            zhats = np.array([est.z for est in estimators])\n",
    "            new_qhat = np.zeros(qhat.shape)\n",
    "            local_FIM = np.zeros(F_est.shape)\n",
    "            for i in G.nodes():\n",
    "                N_i = [i]+list(G[i]) \n",
    "                # Estimate\n",
    "                if consensus_est:\n",
    "                    inv_d_neighbor=inv_d[N_i]\n",
    "                else:\n",
    "                    inv_d_neighbor=None\n",
    "                new_qhat[i,:]=estimators[i].update_and_estimate_loc(hs[i],dhdzs[i],y[N_i],p[N_i],zhats[N_i],consensus_weights=inv_d_neighbor)\n",
    "                if not coordinate:\n",
    "                    local_FIM[i,:,:]=analytic_FIM(qhat[N_i,:],p[N_i],C1,C0,k,b)\n",
    "\n",
    "            qhat=new_qhat\n",
    "\n",
    "            # Partial FIM Calculation and FIM consensus\n",
    "            for _ in range(FIM_cons_iter):\n",
    "                new_F = joint_F_single(qhat,p)\n",
    "                dF = new_F-F\n",
    "                F=new_F\n",
    "\n",
    "                # FIM Consensus using parallel two-pass algorithm\n",
    "                inv_d = W.dot(inv_d)\n",
    "                w_F_est = (w_F_est.T.dot(W)).T + dF*inv_d[:,np.newaxis,np.newaxis]\n",
    "                F_est = w_F_est/inv_d[:,np.newaxis,np.newaxis]   \n",
    "\n",
    "            # Gradient update\n",
    "            FIM_cand= F_est if coordinate else local_FIM\n",
    "            for i in range(N_sen):\n",
    "                dp=analytic_dLdp(qhat[i:i+1],p[i:i+1],C1,C0,k,b,FIM=FIM_cand[i])\n",
    "\n",
    "                p[i:i+1]-=max_linear_speed*dp/np.linalg.norm(dp)\n",
    "\n",
    "            # Record data\n",
    "            p_history.append(np.array(p))\n",
    "            qhat_history.append(np.array(qhat))\n",
    "            q_history.append(np.array(q))\n",
    "        \n",
    "        return {'p':p_history,'qhat':qhat_history,'q':q_history}\n",
    "        \n",
    "        \n",
    "    output = Parallel(n_jobs = 10)([delayed(simulate_one_trial)() for _ in range(N_trails)])\n",
    "\n",
    "    \n",
    "    data={'p':[],'qhat':[],'q':[]}\n",
    "    \n",
    "    for o in output:\n",
    "        for key, item in o.items():\n",
    "            data[key].append(item)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4v/bq7m33xj6b9fm2jfm0ngdg3c0000gn/T/ipykernel_1870/145514847.py:19: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A = np.array(nx.adjacency_matrix(G).todense().astype(float))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (3,1,1) and (3,4) not aligned: 1 (dim 2) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/tianpengzhang/opt/miniconda3/envs/main/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 428, in _process_worker\n    r = call_item()\n  File \"/Users/tianpengzhang/opt/miniconda3/envs/main/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/Users/tianpengzhang/opt/miniconda3/envs/main/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/tianpengzhang/opt/miniconda3/envs/main/lib/python3.10/site-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n  File \"/Users/tianpengzhang/opt/miniconda3/envs/main/lib/python3.10/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/var/folders/4v/bq7m33xj6b9fm2jfm0ngdg3c0000gn/T/ipykernel_1870/145514847.py\", line 122, in simulate_one_trial\n  File \"/Users/tianpengzhang/Dropbox (Harvard University)/Tianpeng's research/Tianpeng & Lina/Distributed Source Seeking/fim_track/Notebooks/../utils/ConsensusEKF.py\", line 92, in update_and_estimate_loc\n    self.update(h,dhdz,y,p,z_neighbor,z_neighbor_bar,consensus_weights)\n  File \"/Users/tianpengzhang/Dropbox (Harvard University)/Tianpeng's research/Tianpeng & Lina/Distributed Source Seeking/fim_track/Notebooks/../utils/ConsensusEKF.py\", line 85, in update\n    self.z= consensus_weights.dot(z_neighbor)/np.sum(consensus_weights) # The consensus term.\nValueError: shapes (3,1,1) and (3,4) not aligned: 1 (dim 2) != 3 (dim 0)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m data \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key,item \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 32\u001b[0m     data[key]\u001b[38;5;241m=\u001b[39m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Data/Distributed-FIM-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mSensor.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(N_sen)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "Cell \u001b[0;32mIn [7], line 154\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(q, N_trails, N_sen, N_iter, consensus_est, coordinate, version, FIM_cons_iter)\u001b[0m\n\u001b[1;32m    149\u001b[0m         q_history\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(q))\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m:p_history,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqhat\u001b[39m\u001b[38;5;124m'\u001b[39m:qhat_history,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m:q_history}\n\u001b[0;32m--> 154\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimulate_one_trial\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mN_trails\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m:[],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqhat\u001b[39m\u001b[38;5;124m'\u001b[39m:[],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m:[]}\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/main/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/main/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/main/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/main/lib/python3.10/concurrent/futures/_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/main/lib/python3.10/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,1,1) and (3,4) not aligned: 1 (dim 2) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel,delayed\n",
    "\n",
    "\n",
    "\n",
    "def get_args(N_sen):\n",
    "    return {'Coord.+Consensus Est.': (q,N_trails,N_sen,N_iter,True,True,'v1', 1),\n",
    "       'No Coord.+Local Est.':(q,N_trails,N_sen,N_iter,False,False,'v1'),\n",
    "       'No Coord.+Consensus Est.':(q,N_trails,N_sen,N_iter,True,False,'v1')}\n",
    "\n",
    "  \n",
    "SEED = 45\n",
    "np.random.seed(SEED)\n",
    "\n",
    "N_trails = 100\n",
    "N_iter = 150\n",
    "FIM_cons_iter = 1\n",
    "\n",
    "q = np.array([6,6])  \n",
    "\n",
    "sen_iter = [4,10,20,40]\n",
    "\n",
    "# sen_iter = [4,6,8]\n",
    "\n",
    "for N_sen in sen_iter:\n",
    "    t = time.time()\n",
    "   \n",
    "    args = {'Coord.+Consensus Est.': (q,N_trails,N_sen,N_iter,True,True,'v1', 1),\n",
    "           'No Coord.+Local Est.':(q,N_trails,N_sen,N_iter,False,False,'v1'),\n",
    "           'No Coord.+Consensus Est.':(q,N_trails,N_sen,N_iter,True,False,'v1')}\n",
    "    data = {}\n",
    "    for key,item in args.items():\n",
    "        data[key]=main(*item)\n",
    "\n",
    "\n",
    "    filepath = \"../Data/Distributed-FIM-{}Sensor.pkl\".format(N_sen)\n",
    "    with open(filepath,'wb') as file:\n",
    "        pkl.dump(data,file)\n",
    "\n",
    "    print('Time',time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralized Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralized_main(q,N_trails,N_sen,N_iter):\n",
    "    '''Experiment Parameters'''\n",
    "  \n",
    "    C_gain=0.1\n",
    "\n",
    "    # Set up virtual sensors\n",
    "    C1=-0.3 # Setting C1 as a negative number mitigates the blowing-up effect when the sensors are close to the source.\n",
    "    C0=0\n",
    "    k=1\n",
    "    b=-2\n",
    "   \n",
    "    noise_std = 0.01\n",
    "    minimum_sensing_reading=1e-5\n",
    "\n",
    "    # The step size of each sensor\n",
    "    max_linear_speed=0.1\n",
    "\n",
    "    # Terminal condition\n",
    "    contact_radius = 0.1\n",
    "\n",
    "\n",
    "    t=time.time()\n",
    "    data={'p':[],'qhat':[],'q':[]}\n",
    "\n",
    "    for _ in range(N_trails):\n",
    "\n",
    "        '''Initialize Key Data Structures'''\n",
    "        p_0 = np.random.rand(N_sen,2)\n",
    "        # qhat_0 = np.array([3,3])\n",
    "        \n",
    "        qhat_0 = np.array([4,2])\n",
    "        # qhat_0 = np.random.rand(2)\n",
    "        \n",
    "        p = np.array(p_0) # Sensor Positins\n",
    "        qhat = np.array(qhat_0)\n",
    "\n",
    "        # The Centralized EKF\n",
    "        estimator = CentralizedEKF(qhat_0)\n",
    "\n",
    "        C1s=C1*np.ones(N_sen)\n",
    "        C0s = C0*np.ones(N_sen)\n",
    "        ks = k * np.ones(N_sen)\n",
    "        bs = b*np.ones(N_sen)\n",
    "\n",
    "        h=partial(joint_meas_func,C1s,C0s,ks,bs)# Freeze the coefficients, the signature becomes h(z,ps))\n",
    "        dhdz=partial(analytic_dhdz,C1s=C1s,C0s=C0s,ks=ks,bs=bs)\n",
    "        dhdqs=partial(analytic_dhdq,C1s=C1s,C0s=C0s,ks=ks,bs=bs)\n",
    "\n",
    "        '''Main Loop'''\n",
    "\n",
    "        p_history = []\n",
    "        qhat_history = []\n",
    "        q_history = []\n",
    "        for _ in range(N_iter):\n",
    "#             if np.min(np.linalg.norm(p-q,axis=1))>contact_radius:#Move and estimate only when not touching the source.\n",
    "            # Measure\n",
    "            r = np.linalg.norm(q-p,axis=1)\n",
    "            y = k* ((r-C1)**b)+C0 + np.random.randn(N_sen)*noise_std\n",
    "            y[y<=0]=minimum_sensing_reading # We don't want y to be zero or negative.\n",
    "\n",
    "            # Estimate\n",
    "\n",
    "            qhat=estimator.update_and_estimate_loc(h,dhdz,y,p)\n",
    "\n",
    "            # Gradient update\n",
    "\n",
    "            dp=analytic_dLdp(qhat,p,C1,C0,k,b)\n",
    "            p-=max_linear_speed*(dp.T/np.linalg.norm(dp,axis=1)).T\n",
    "\n",
    "            # Record data\n",
    "            p_history.append(np.array(p))\n",
    "            qhat_history.append(np.array(qhat))  \n",
    "            q_history.append(np.array(q))\n",
    "\n",
    "            # Check terminal condition\n",
    "            \n",
    "        data['q'].append(q_history)\n",
    "        data['p'].append(np.array(p_history))\n",
    "        data['qhat'].append(np.array(qhat_history))\n",
    "\n",
    "    print('Time:',time.time()-t)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.6099860668182373\n",
      "Time: 3.562819004058838\n",
      "Time: 4.115206003189087\n",
      "Time: 4.677908897399902\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel,delayed\n",
    "\n",
    "def simulate(N_sen,N_trails,N_iter):\n",
    "    \n",
    "    SEED = 45\n",
    "    np.random.seed(SEED)\n",
    "    data={}\n",
    "    data['centralized']= centralized_main(q,N_trails,N_sen,N_iter)\n",
    "    filepath = \"../Data/CentralizedData-{}Sensor.pkl\".format(N_sen)\n",
    "    with open(filepath,'wb') as file:\n",
    "        pkl.dump(data,file)\n",
    "\n",
    "\n",
    "\n",
    "q = np.array([6,6])  \n",
    "N_trails = 100\n",
    "N_iter = 150\n",
    "\n",
    "\n",
    "for N_sen in [4,10,20,40]:\n",
    "    simulate(N_sen,N_trails,N_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
